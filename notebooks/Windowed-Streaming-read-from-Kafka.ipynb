{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook with Kafka producer and Spark Streaming Processor\n",
    "## With Windowed processing\n",
    "\n",
    "<img src=\"work/image2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import sys, os, json\n",
    "\n",
    "# to get the appropriate version of spark\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#    Spark\n",
    "from pyspark import SparkContext\n",
    "#    Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "#    Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'alerts'\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "#         print( ' Message published successfully.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka', ex)\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('work/alerts.csv');\n",
    "print(df.head(2))\n",
    "kafka_producer = connect_kafka_producer()\n",
    "\n",
    "#  iterate over the alerts csv file and send alerts as the value, and keys can be the alert uuid\n",
    "for index, row in df.iterrows():\n",
    "    publish_message(kafka_producer, topic_name, 'alert',  row.to_json())\n",
    "if kafka_producer is not None:\n",
    "    kafka_producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up environment\n",
    "# Important when using in jupyter notebook, since we are not submitting the job via command line\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.0.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparkWindowedContext():\n",
    "    # Create SPARK Context\n",
    "    sc = SparkContext(appName=\"PythonSparkWindowedStreamingKafka\")\n",
    "    sc.setLogLevel(\"WARN\")\n",
    "    ## Create Streaming context , with 5 second interval \n",
    "    ssc = StreamingContext(sc,  5) \n",
    "\n",
    "    ### Connect to KAFKA\n",
    "    ### consumer group id = spark-streaming \n",
    "    ### zookeeper quorum = localhost: 2181 \n",
    "    ### topic: 'alerts', use 1 cluster for this topic, so {alerts: 1}\n",
    "\n",
    "    #  After a context is defined, you have to do the following.\n",
    "    # Define the input sources by creating input DStreams. here KafkaStream is a Dtream type object\n",
    "\n",
    "    kafkaStream = KafkaUtils.createStream(ssc, 'localhost:2181', 'spark-streaming', {topic_name: 1})\n",
    "\n",
    "    # Count number of tweets in the batch\n",
    "    count_this_batch = kafkaStream.count().map(lambda x:('Alert count this batch: %s' % x))\n",
    "    \n",
    "    # Count by windowed time period\n",
    "    alert_count_windowed = kafkaStream.countByWindow(20, 5).map(lambda x:('Total alerts (One minute rolling count): %s' % x))\n",
    "    \n",
    "    # Extract data\n",
    "    parsed = kafkaStream.map(lambda v: json.loads(v[1]))\n",
    "    \n",
    "    # Get alerts status \n",
    "    source_dstream = parsed.map(lambda alert: alert['src'])\n",
    "    \n",
    "    # Count each value and number of occurences \n",
    "    count_src_this_batch = source_dstream.countByValue().transform(lambda rdd: rdd).map(lambda x:\"Source counts this batch:\\tValue %s\\tCount %s\" % (x[0],x[1]))\n",
    "\n",
    "    # for the window \n",
    "    count_src_windowed = source_dstream.countByValueAndWindow(60,5) .transform(lambda rdd:rdd).map(lambda x:\"Alert source counts (One minute rolling):\\tValue %s\\tCount %s\" % (x[0],x[1]))\n",
    "                                           \n",
    "    # Write tweet Status counts to stdout\n",
    "    count_src_this_batch.pprint(5)\n",
    "    count_src_windowed.pprint(5)     \n",
    "    \n",
    "    # Done with a union here instead of two separate pprint statements just to make it cleaner to display\n",
    "    count_this_batch.union(alert_count_windowed).pprint() \n",
    "\n",
    "    return ssc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc = StreamingContext.getOrCreate('work/checkpoint_wednesday', lambda: sparkWindowedContext())\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
